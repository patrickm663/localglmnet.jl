# localglmnet.jl
This is a WIP implementation of some of the work found in Richman &amp; WuÌˆthrich ([2021](https://arxiv.org/pdf/2107.11059.pdf)), using Julia's Flux.jl + CUDA.jl.

While this does not aim to fully replicate the results of the paper, it follows some of its principles and shows some methodology for interpreting neural nets.

## Data

The data is synthesised using the following equation, taken from Richman et al.:

$$f(\mathbf{x}) = \frac{1}{2}x_1 - \frac{1}{4}x_2^2 + \frac{1}{2}|x_3|\sin(2x_3) + \frac{1}{2}x_4 x_5 + \frac{1}{8}x_{5}^2x_{6}$$

As an input, we create a 100 000 x 8 matrix of independent observations from a standard normal distribution, with the cavaet that features $x_2$ and $x_8$ have a 50% correlation.

The benefit of testing out a synthetic problem is that we have a reference point to validate against. The above function is also not 'trivial' like solving $f(x) = 2 + 4x$, and has the interesting catch that not all of the input features actually influence the final result.

## Approach

We train a feed-forward neural network to approximate the above equation. 75% of the data is used for training and the remainder for validation (all from the same sample). Once fitted, we test it on a further 100 000 observations, generated by taking new samples.

Once fitted, we construct naive partial dependence plots by isolating the effect a single feature $x_i$ has on the output by passing only feature $x_i$ through the NN -- zeros everywhere else. This gives us a rough approximation for the impact changes in feature $x_i$ will have on the output.

We then add the outputs from the isolated outputs to form an additive decomposition of the NN -- including any bias term.

Finally, we find an analytical approximation for the NN using `SymbolicRegression.jl`.

## Results

The MSE over 7 500 epochs:

TO ADD

The MSE for the training, validation, and testing sets:

TO ADD

We calulate the Jacobian after passing a single vector of data through the NN and the actual model. The results indicate the gradients are fairly similar, which means the NN picked up the underlying relation fairly well.

As a rough measure of variable importance, we take the average of the isolated effects and rank them by absolute magnitude.

TO ADD
## Acknowledgements

The full print of Richman et al. is available at: https://arxiv.org/pdf/2107.11059.pdf
